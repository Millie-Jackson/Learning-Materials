{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IiZ2wgrqQ44"
      },
      "source": [
        "# PyTorch\n",
        "\n",
        "\n",
        "## General\n",
        "\n",
        "> An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n",
        "- Mostly used to create neural networks\n",
        "- Specialized to use hardware acceleration (GPU, TPU, Tensor Cores etc.)\n",
        "- __API based on `numpy`__ (`torch.Tensor` instead of `np.array`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYs3Q5IKqQ48"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmh-pk8tqQ49",
        "outputId": "440a3738-0575-40a0-ffaa-0e347850810d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.float32 torch.Size([300])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "# Alias below is pretty common\n",
        "# one can also use torch directly\n",
        "import torch.nn.functional as F\n",
        "\n",
        "X = torch.rand(300, 10)  # Random uniform\n",
        "print(type(X))\n",
        "\n",
        "W = torch.randn(10)  # Random normal\n",
        "b = torch.tensor([1])  # create again another random tensor\n",
        "\n",
        "y = X @ W + b\n",
        "\n",
        "print(y.dtype, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHLfkA8XqQ5A"
      },
      "source": [
        "## Data types\n",
        "\n",
        "Similarly to `numpy`, PyTorch provides multiple data types, for example: \n",
        "\n",
        "- `torch.float` (32-bit precision)\n",
        "- `torch.double` (64-bit precision)\n",
        "- `torch.half` (16-bit precision)\n",
        "\n",
        "and many others (see [here](https://pytorch.org/docs/stable/tensor_attributes.html)).\n",
        "\n",
        "> Usually we will use floating point values (either `float` or `half`), depending on context\n",
        "\n",
        "### Why not double?\n",
        "\n",
        "> Default `dtype` in PyTorch is `float` because __it doesn't take up so much memory and is accurate enough__\n",
        "\n",
        "Also, GPU memory is costly (and there isn't enough of it usually), hence lower precision (up to a certain point) might be a good solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UruTFFJyqQ5B"
      },
      "source": [
        "## Casting\n",
        "\n",
        "One can easily cast PyTorch tensors to desired data types, see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA8tRHLPqQ5B",
        "outputId": "a56625b1-3393-4dc4-a0cb-65b5951cfd16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float64 torch.float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "array = np.random.randn(10, 5)\n",
        "tensor = torch.from_numpy(array)\n",
        "print(array.dtype, tensor.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNrx9UyeqQ5C",
        "outputId": "eea968db-385a-4aeb-de1c-7a34796b7004"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float16"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cast to half type\n",
        "new_tensor = tensor.half()\n",
        "\n",
        "new_tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJtq1f0qQ5D",
        "outputId": "b6aad21e-75a5-4692-9a4a-d6000f2f572c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.2207 , -0.4895 , -0.974  , -0.7163 , -0.3904 ],\n",
              "       [ 0.261  , -0.1913 ,  0.09644, -0.4658 ,  1.246  ],\n",
              "       [-0.2954 ,  0.4834 ,  0.8955 , -0.6025 , -1.111  ],\n",
              "       [-0.3735 , -0.6553 ,  1.651  ,  0.1934 , -0.1555 ],\n",
              "       [ 0.5166 , -0.6772 , -0.3328 , -1.351  ,  1.748  ],\n",
              "       [-0.1812 ,  0.512  , -0.475  , -0.4182 , -0.2444 ],\n",
              "       [ 1.421  , -0.3674 , -0.2566 , -0.2686 ,  1.169  ],\n",
              "       [ 0.4937 , -1.168  , -0.0638 , -0.03564, -0.572  ],\n",
              "       [-1.131  ,  1.469  ,  0.3977 ,  1.199  , -0.5757 ],\n",
              "       [-0.648  , -0.01033, -0.608  , -0.2213 , -0.4197 ]], dtype=float16)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# numpy interoperability\n",
        "new_tensor.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU9j5OCtqQ5E",
        "outputId": "ae056db2-b87c-4e60-8e97-1f60ec7317a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# upcasting\n",
        "(new_tensor + tensor).dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSY4lGRqqQ5F"
      },
      "source": [
        "## Device\n",
        "\n",
        "PyTorch can utilize multiple device types. In general:\n",
        "- we use CPU for data loading\n",
        "- we use specialized devices (usually GPU, sometimes TPU) for running the data through neural network\n",
        "\n",
        "> TPU support is currently experimental, see challenges for more info\n",
        "\n",
        "Let's start by checking if GPU is available on our devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yho9GWWqQ5G",
        "outputId": "4adec452-09b2-4ab3-a4c7-04ca81d29491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCW1hrcoqQ5H"
      },
      "source": [
        "Based on this information we can create a special device type that we can later use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqZ_Gdz5qQ5H",
        "outputId": "6bf64073-e2e3-4fac-e36b-93f124a036d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sqQyNNmqQ5I"
      },
      "source": [
        "In later sections (basics of training) you will see how we can use this device variable\n",
        "for device agnostic code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "rnJlojqDqQ5I"
      },
      "source": [
        "## Automatic differentiation\n",
        "\n",
        "In order for neural networks to learn we need to calculate gradients of `loss` w.r.t. parameters (like we did with linear regression previously).\n",
        "\n",
        "> This time differentiation graph (__sometimes also called a tape__) is provided by PyTorch\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Deep-Learning/2.%20Intro%20to%20PyTorch/0.%20What%20is%20PyTorch%3F/images/grad.jpg?raw=1)\n",
        "\n",
        "To use PyTorch's [autograd](https://pytorch.org/docs/stable/autograd.html) we need a few changes in the above code.\n",
        "\n",
        "First, we have to mark tensors which require gradient using `requires_grad=True` argument during creation:\n",
        "\n",
        "> Most of PyTorch functions creating tensors like `rand`, `randn` etc. have `requires_grad` as an optional parameter!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrocIbZhqQ5J"
      },
      "outputs": [],
      "source": [
        "W = torch.randn(10, requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouDws5z1qQ5J"
      },
      "source": [
        "> Only tensor of floating data type can have gradient! __No integers or a-like__\n",
        "\n",
        "After that we can use them normally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M3OSIYAqQ5J"
      },
      "outputs": [],
      "source": [
        "y = X @ W + b\n",
        "\n",
        "loss = y.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USo5-RIRqQ5K"
      },
      "source": [
        "## Running backpropagation\n",
        "\n",
        "Like we did during \"Gradient Methods\" we can run backpropagation algorithm explicitly.\n",
        "\n",
        "> In PyTorch we run backpropagation __on tensor__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t52ZOdeAqQ5K",
        "outputId": "80f047ef-3768-484a-9c6a-9147fc987c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None None\n",
            "tensor([147.7014, 155.7213, 148.9580, 143.3640, 146.8089, 154.9344, 145.1965,\n",
            "        145.6324, 139.8669, 152.7871]) tensor([300.])\n"
          ]
        }
      ],
      "source": [
        "print(W.grad, b.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# Use .grad attribute \n",
        "print(W.grad, b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWpVyFEFqQ5K"
      },
      "source": [
        "> Implicitly tensor with `1` is fed into `backward` __if tensor is a scalar!__\n",
        "\n",
        "> If tensor is not a scalar, you have to provide a tensor with initial gradient of specified shape, see [here](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ2yUau2qQ5L"
      },
      "source": [
        "## grad_fn, how PyTorch keeps track of operations\n",
        "\n",
        "- __PyTorch keeps functions which created the tensor (if any) inside `grad_fn`__ attribute\n",
        "- if `grad_fn` is `None` it is a tensor which:\n",
        "    - was created by user explicitly (either with `requires_grad` set to `True` or `False`)\n",
        "    \n",
        "See below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H6w2zoxqQ5L",
        "outputId": "7ea93ce6-eb9c-474a-d812-253ce772379a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x0000022884966EE0> False True\n",
            "None True True\n",
            "None True False\n"
          ]
        }
      ],
      "source": [
        "print(y.grad_fn, y.is_leaf, y.requires_grad)\n",
        "\n",
        "print(W.grad_fn, W.is_leaf, W.requires_grad)\n",
        "\n",
        "print(X.grad_fn, X.is_leaf, X.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTBZoorZqQ5M"
      },
      "source": [
        "# Modules\n",
        "\n",
        "> __PyTorch provides multiple modules based on what we intend to do__\n",
        "\n",
        "> __REFER TO THIS LIST WHENEVER IN DOUBT__\n",
        "\n",
        "> __MOST OF NUMPY FUNCTIONALITY IS IMPLEMENTED HENCE THERE IS NO NEED TO USE IT, ALWAYS USE PYTORCH COUNTERPARTS!__\n",
        "\n",
        "> __DO NOT USE PYTHON BUILT-IN FUNCTIONS LIKE SUM!__\n",
        "\n",
        "## [import torch](https://pytorch.org/docs/stable/torch.html) \n",
        "\n",
        "> __Basic functionality, acts like `numpy` high level namespace__\n",
        "\n",
        "- creating tensors, like:\n",
        "    - `torch.tensor`- creates tensor from `list` data (like `np.array`)\n",
        "    - `torch.randn` - random normal tensor of specified shape\n",
        "    - `torch.zeros`, `torch.zeros_like`\n",
        "- indexing, slicing, like:\n",
        "    - `torch.cat` - concatenate across __given dimension__\n",
        "    - `torch.stack` - stack across __new dimension__\n",
        "    - `torch.reshape` - reshape the tensor (use instead of `torch.view`)\n",
        "- random sampling (`torch.seed`)\n",
        "- mathematical functions like \n",
        "    - `torch.sin` \n",
        "    - `torch.sigmoid`\n",
        "    - `torch.abs`\n",
        "    - `torch.mean`\n",
        "- reduction operations, like:\n",
        "    - `torch.argmax` - __index of item with maximum value__ (possibly across dimensions)\n",
        "    - `torch.min` - minimum value (possibly across dimensions)\n",
        "    - `torch.var_mean` - variance and mean (possibly across dimensions)\n",
        "- comparison operations, like:\n",
        "    - `torch.top_k` - maximum `k` values (and their indices)\n",
        "    - `torch.sort` - sort values (possibly across dimensions)\n",
        "    - `torch.argsort` - as above, but return indices sorting the tensor\n",
        "- enabling/disabling gradient (`torch.no_grad`, see training & loop section)\n",
        "- __other operations on tensors__\n",
        "\n",
        "Let's see some of them in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:29:56.283399Z",
          "start_time": "2021-06-20T18:29:54.586133Z"
        },
        "id": "9hho0uS_qQ5M",
        "outputId": "b91abda5-eaa5-4bff-83f1-4767033e1551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.int64, torch.float32)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([1, 2, 3]) # use torch.tensor always as it does type inference\n",
        "t2 = torch.Tensor([1, 2, 3])\n",
        "\n",
        "t1. dtype, t2.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:31:00.262569Z",
          "start_time": "2021-06-20T18:31:00.221408Z"
        },
        "id": "c1ckwpCvqQ5N",
        "outputId": "78116e89-8368-496f-8295-8ab9506f41b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.4164,  0.3896, -0.2437,  0.1109,  0.1156,  0.7962, -0.9374,  0.5662,\n",
              "         0.1221])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random tensor of shape and slicing it\n",
        "\n",
        "t1 = torch.zeros(64, 18)\n",
        "t2 = torch.randn_like(t1)\n",
        "t2[0, 9:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:32:07.588426Z",
          "start_time": "2021-06-20T18:32:07.582375Z"
        },
        "id": "5pywd_znqQ5N",
        "outputId": "821194f9-4803-46c1-c179-f9e9efee15f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([[2.5645, 2.4702, 2.9204, 2.1125, 2.4508, 3.1817, 2.4998, 2.2284, 2.1669,\n",
              "         2.1737, 2.0773, 4.0561, 1.7559, 2.7503, 2.2525, 1.6442, 3.5927, 1.9406],\n",
              "        [2.5129, 2.4029, 2.1267, 1.9612, 1.6177, 2.2472, 1.4460, 2.2186, 2.0504,\n",
              "         1.6029, 1.9764, 2.4437, 1.5435, 2.3339, 2.1958, 1.5503, 2.2064, 1.6870],\n",
              "        [2.3649, 1.9801, 1.4681, 1.4865, 1.6087, 1.9867, 1.4129, 1.9937, 1.9930,\n",
              "         1.5907, 1.9118, 2.3072, 1.4368, 2.2790, 1.8168, 1.4041, 2.0703, 1.4445]]),\n",
              "indices=tensor([[ 5, 29, 42, 23, 26, 14, 63, 30, 51,  6, 53, 34, 56, 13, 38, 60, 52, 57],\n",
              "        [31, 17,  1, 41, 36, 47,  7, 31, 30, 14, 21, 52,  7, 47, 24, 59, 33, 56],\n",
              "        [ 3, 42, 32,  2, 15, 10, 48, 23, 20,  3,  8, 11,  1, 62, 52, 32, 16, 48]]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.topk(t2, k=3, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:33:32.013183Z",
          "start_time": "2021-06-20T18:33:32.002295Z"
        },
        "id": "Nm-b4a2VqQ5N",
        "outputId": "7323f8a3-5a78-4e87-95c0-61f3435ad701"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 3])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1 = torch.tensor([1, 2, 3])\n",
        "torch.stack((t1, t1, t1, t1), dim=0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq9aw3o8qQ5O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:33:34.286154Z",
          "start_time": "2021-06-20T18:33:34.273185Z"
        },
        "id": "ec1QZ2d3qQ5O",
        "outputId": "22a9056f-91aa-4166-e38e-c19369bbb32f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([9])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cat((t1, t1 ,t1), dim=0).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJwWHKvlqQ5P"
      },
      "source": [
        "## [import torch.nn as nn](https://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "> __PyTorch neural network related modules__\n",
        "\n",
        "We will learn more about that in the following lessons, __but keep in mind that__:\n",
        "- __All of the operations (layers) are presented as classes (which you instantiate)__\n",
        "- __Layers can be mixed together and depend on `torch.nn.Module` (ALL OF THEM ARE INSTANCES OF IT__)\n",
        "\n",
        "Some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0KHkKgRqQ5Q"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "layer = nn.Linear(10, 5)\n",
        "for param in layer.parameters():\n",
        "    print(param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzIkrFt2qQ5Q"
      },
      "outputs": [],
      "source": [
        "# Layers are callable functors\n",
        "\n",
        "another_layer = nn.Conv3d(12, 32, 3)\n",
        "inputs = torch.randn(64, 12, )\n",
        "outputs = another_layers(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX4A9Q5cqQ5Q"
      },
      "source": [
        "## [import torch.nn.functional as F](https://pytorch.org/docs/stable/nn.functional.html)\n",
        "\n",
        "> __PyTorch neural network components PROVIDED AS FUNCTIONS INSTEAD OF OBJECTS__\n",
        "\n",
        "Why would we use one over the other? __Code readability__ is the answer.\n",
        "\n",
        "First, let's see where `torch.nn` looks better than `torch.nn.fucntional`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:57:33.801051Z",
          "start_time": "2021-06-20T18:57:33.796280Z"
        },
        "id": "5DC-cj87qQ5R"
      },
      "outputs": [],
      "source": [
        "# Setup cell with data\n",
        "\n",
        "data = torch.randn(64, 10) # 64 examples, 10 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:57:55.378884Z",
          "start_time": "2021-06-20T18:57:55.374729Z"
        },
        "id": "T_GygZX9qQ5R",
        "outputId": "3180ebfa-ca6b-4d5f-930d-bd6fd74f5dd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 5])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn approach\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "layer = nn.Linear(10, 5)\n",
        "output = layer(data)\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T18:57:57.563084Z",
          "start_time": "2021-06-20T18:57:57.559437Z"
        },
        "id": "KuAD9t6SqQ5R",
        "outputId": "126448da-f9fc-4f66-8876-cf33012e7b62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 5])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional approach\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "output = F.linear(data, weight=torch.randn(5, 10))\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIu5aGTTqQ5R"
      },
      "source": [
        "### Pros of torch.nn\n",
        "\n",
        "- __Creating stateful objects (NEURAL NETWORKS AND OTHER MODELS)__\n",
        "- Implicitly creates weight (with appropriate initialization!)\n",
        "- Easier to compose and divided steps\n",
        "\n",
        "Now let's see when `torch.nn.functional` is better:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T19:00:51.871514Z",
          "start_time": "2021-06-20T19:00:51.861078Z"
        },
        "id": "dQ64Z9GSqQ5S",
        "outputId": "08e53ddd-22db-48fe-cb48-442e7b3b9203"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 10])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn approach\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "probabilities = softmax(data)\n",
        "\n",
        "probabilities.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-06-20T19:01:19.607523Z",
          "start_time": "2021-06-20T19:01:19.598416Z"
        },
        "id": "ZOZLKwiTqQ5S",
        "outputId": "d033530c-3218-4db3-f918-2336d567afe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 10])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional approach\n",
        "\n",
        "probabilities = F.softmax(data, dim=1)\n",
        "\n",
        "probabilities.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EGCtYmBqQ5S"
      },
      "source": [
        "### Pros of torch.nn.functional\n",
        "\n",
        "- __Using non-stateful objects__ (softmax has no weights or attributes)\n",
        "- Directly applicable\n",
        "- __Used only one time__ (and reused when needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2noWJlUqQ5S"
      },
      "source": [
        "## torch.nn.Module\n",
        "\n",
        "> [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is a base class for every deep learning model in PyTorch (usually neural networks)\n",
        "\n",
        "Given that, we will inherit it from it each time we create a more complicated module.\n",
        "\n",
        "Let's see how we can code up __linear regression__:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5LsqxNHqQ5S"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(torch.nn.Module):\n",
        "    def __init__(self, n_features: int):\n",
        "        # This line is always required at the beginning\n",
        "        # Registers parameters of our model in graph\n",
        "        super().__init__()\n",
        "\n",
        "        self.W = torch.nn.Parameter(torch.randn(n_features))\n",
        "        self.b = torch.nn.Parameter(torch.ones(1))\n",
        "        self.other_tensor = torch.randn(5)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X @ self.W + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7yY6qjtqQ5T"
      },
      "source": [
        "### torch.nn.Parameter\n",
        "\n",
        "> If we want a tensor to be a part of `nn.Module` we have to wrap it inside `nn.Parameter`\n",
        "\n",
        "Let's see what parameters our model currently has:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B1mUKLRqQ5T",
        "outputId": "27b60adc-de9d-431b-980e-14c1bf75dd74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W torch.Size([15])\n",
            "b torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "model = LinearRegression(15)\n",
        "\n",
        "# named_parameters is a generator, you can also use parameters method\n",
        "for name, parameter in model.named_parameters():\n",
        "    print(name, parameter.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCnvy6CoqQ5T"
      },
      "source": [
        "As one can see `self.other_tensor` __is not registered as a parameter__.\n",
        "\n",
        "This means, we won't be able to easily optimize it and it is \"merely an attribute\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnEptH8hqQ5T"
      },
      "source": [
        "## forward method\n",
        "\n",
        "Users should implement logic of the model (how data goes through neural network) inside this method.\n",
        "\n",
        "> When running data through our model we will use `__call__` method. __This ensures any hooks registered for module will run correctly__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3NCOfR2qQ5U",
        "outputId": "7c2662dd-2256-43c7-c078-561468d154d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = model(torch.randn(64, 15))\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi1jD8gkqQ5U"
      },
      "source": [
        "## Input shape\n",
        "\n",
        "> PyTorch requires `(batch_size, n_features1, ..., n_features2)` tensors as input\n",
        "\n",
        "In the case above, batch size was `64` with `15` input features to linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhKthLBcqQ5U"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Create __multiclass LogisticRegression__ layer from scratch by inheriting from `nn.Module` and using `nn.Parameter`.\n",
        "\n",
        "- User can specify number of input features and `n_classes` in the initialization\n",
        "- Remember about the `forward` method (__returns logits__)\n",
        "- Create another method `predict_proba` by reusing code above (which PyTorch module should you use?)\n",
        "- Create another method `predict` which returns correct class __for each sample__ (use output from __correctly called__ `forward` method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaDi4F3GqQ5U"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, in_features: int, n_classes: int):\n",
        "        super().__init__()\n",
        "        self.in_features: int = in_features\n",
        "        self.n_classes: int = n_classes\n",
        "\n",
        "        # W: (in_features, n_classess)\n",
        "        self.W = torch.nn.Parameter(torch.randn(self.in_features, self.n_classes))\n",
        "        self.b = torch.nn.Parameter(torch.ones(self.n_classes))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X: (batch, in_features)\n",
        "        return X @ self.W + self.b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return torch.nn.functional.softmax(self(X), dim=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return torch.argmax(self(X), dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEUnZeEoqQ5U"
      },
      "source": [
        "## Summary\n",
        "\n",
        "- PyTorch can be considered as `numpy` on GPU for neural networks\n",
        "- PyTorch provides different data types:\n",
        "    - `float` is a good default value (good balance between necessary precision, performance and memory usage)\n",
        "- PyTorch can run on different devices:\n",
        "    - GPU is used for running neural networks\n",
        "    - CPU is used for data loading and other intensive tasks\n",
        "- We should write device agnostic code (basics shown here)\n",
        "- We should inherit from `torch.nn.Module` when creating neural networks\n",
        "    - Override `__init__` and `forward`\n",
        "    - Use this model via functor `__call__`\n",
        "\n",
        "\n",
        "# Challenges\n",
        "\n",
        "## Assessment\n",
        "\n",
        "- Check available function and attributes of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). What are those used for, see some examples.\n",
        "\n",
        "## Non-assessment\n",
        "\n",
        "- Check out [CUDA semantics](https://pytorch.org/docs/stable/notes/cuda.html) in PyTorch. How to choose specific GPU device if there are multiple of them?\n",
        "- What are the aforementioned hooks? Check out [this article](https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904) for more information"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}